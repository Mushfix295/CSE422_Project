# -*- coding: utf-8 -*-
"""Copy of CSE422_Project_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R8L140-OpOUgSCKLzj2cewb2yjv-QG0W
"""

# run in a fresh cell, then RESTART the runtime
!pip install -q --upgrade "scikit-learn==1.4.2" scikeras==0.13.0

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Dataset Description**"""

from google.colab import files, drive
import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
uploaded = files.upload()
df = pd.read_csv(next(iter(uploaded)))
df.head()

# Clean column names: remove tabs, newlines, and extra spaces
df.columns = df.columns.str.replace(r'[\t\n\r]', '', regex=True).str.strip()

from tabulate import tabulate

TARGET = "Target"
cat_feats = [
    "Marital status",
    "Application mode",
    "Course",
    "Daytime/evening attendance\t",
    "Previous qualification",
    "Nacionality",
    "Mother's qualification",
    "Father's qualification",
    "Mother's occupation",
    "Father's occupation",
    "Displaced",
    "Educational special needs",
    "Debtor",
    "Tuition fees up to date",
    "Gender",
    "Scholarship holder",
    "International"
]

num_feats = [
    "Application order",
    "Previous qualification (grade)",
    "Admission grade",
    "Age at enrollment",
    "Unemployment rate",
    "Inflation rate",
    "GDP"
]
variable_descriptions = {
    "Variable Name": cat_feats + num_feats + [TARGET],
    "Role": ["Feature"] * (len(cat_feats) + len(num_feats)) + ["Target"],
    "Type": (
        ["Categorical"] * len(cat_feats)
        + ["Continuous"] * len(num_feats)
        + ["Categorical"]
    ),
    "Description": [
        "Marital status of student",
        "Application mode",
        "Chosen course",
        "Daytime or evening attendance",
        "Previous qualification type",
        "Nationality of student",
        "Mother's highest qualification",
        "Father's highest qualification",
        "Mother's occupation",
        "Father's occupation",
        "Student displaced (yes/no)",
        "Educational special needs (yes/no)",
        "Debtor status (yes/no)",
        "Tuition fees up to date (yes/no)",
        "Gender of student",
        "Scholarship holder (yes/no)",
        "International student (yes/no)",
        "Application order",
        "Grade of previous qualification",
        "Admission grade",
        "Age at enrollment",
        "Unemployment rate (economic indicator)",
        "Inflation rate (economic indicator)",
        "Gross Domestic Product (GDP indicator)",
        "Final academic outcome (Dropout / Graduate / Enrolled)"
    ],
    "Missing Values": [
        "Yes" if col in df.columns and df[col].isnull().any() else "No"
        for col in cat_feats + num_feats + [TARGET]
    ]
}
desc_df = pd.DataFrame(variable_descriptions)
print("=== VARIABLE DESCRIPTION TABLE ===")
print(tabulate(desc_df, headers="keys", tablefmt="grid", showindex=False))

"""**Infos regarding data set structures and other basics**"""

num_rows, num_cols = df.shape
num_features = num_cols - 1  # excluding target
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("=== Dataset Description ===")
print(f"Total rows: {num_rows}")
print(f"Total columns: {num_cols}")
print(f"Features (excluding target): {num_features}")
print(f"Categorical features: {len(categorical_cols)}")
print(f"Numerical features: {len(numerical_cols)}")
print("\nDataset Description:")
print(df.info())
print("\nSummary statistics:")
print(df.describe())

"""**Types of Features**"""

# Remove empty columns first
df = df.loc[:, ~df.columns.str.contains(r'^Unnamed')]

# Identify categorical & numerical features
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("=== Categorical Features ===")
print(categorical_cols)

print("\n=== Numerical Features ===")
print(numerical_cols)

print("\n=== Classification or Regression Problem? ===")
print("This is a CLASSIFICATION problem because the target column 'Target' "
      "has categorical values (Dropout / Graduate / Enrolled).")

"""#**Missing‑Value count**"""

missing = df.isnull().sum().sort_values(ascending=False)
missing[missing>0]

"""# **Target categories**"""

plt.figure(figsize=(10,8))
sns.countplot(x='Target', data=df)
plt.title('Target Categories')
print(df['Target'].value_counts())
plt.show()

"""

# **Correlation Analysis through Heat-map**"""

from sklearn.preprocessing import LabelEncoder
df_corr = df.copy()
df_corr['Target_num'] = LabelEncoder().fit_transform(df_corr['Target'])

plt.figure(figsize=(20,10))
sns.heatmap(df_corr.corr(numeric_only=True), cmap='coolwarm', annot=True,fmt = '.2f')
plt.title('Correlation Heat map (features and Target variables)')
plt.show()

"""## Correlation Matrix Highlights

### 1. Strong Feature–Feature Links
- **Mother’s vs Father’s occupation** (~ +0.95)  
  These two columns almost duplicate each other—one could be dropped or combined.  
- **Admission grade ↔ Previous qualification (grade)** (~ +0.55)  
  Students’ entry exam scores track their prior academic performance.

### 2. Macro-Economic Signals
- **International ↔ GDP** (~ +0.50)  
  Higher-GDP regions send more international students.  
- **Age at enrollment ↔ Application mode** (~ +0.55)  
  Older students disproportionately use certain application channels.

### 3. Target–Feature Relationships
- **Previous qualification (grade) vs Target** (~ +0.40)  
  Higher prior grades modestly predict successful graduation/enrolment.  
- **International vs Target** (~ +0.30)  
  International status shows a small positive association with the chosen outcome.  
- **Age at enrollment vs Target** (~ –0.30)  
  Older entrants are somewhat less likely to graduate/enrol successfully.

### 4. Weak or Negligible Correlations
Most binary flags (Debtor, Displaced, Scholarship holder) and economic rates (Unemployment, Inflation) show near-zero correlation with each other and with the target—these may carry little predictive power on their own.

# **Exploratory Data Analysis (EDA)**
**5·1 Descriptive Stats**
"""

display(df.describe(include='all').T)

"""Define Numerical and categorical features

**5·2 Numeric Distributions & Outliers**

### **Summary satistics of Numerical Features**
"""

numerical_data = df[num_feats]
numerical_data.describe().T

numerical_data.var()

numerical_data.skew()

"""# Skewness Interpretation

* **Application order (1.871)**: Strong right-skew – most students have lower application order numbers; some have much higher orders.

* **Previous qualification grade (0.307)**: Fairly symmetrical – slight positive skew, close to normal distribution.

* **Admission grade (0.499)**: Moderate right-skew – more students have grades below the mean; some have notably higher grades.

* **Age at enrollment (2.074)**: Strong right-skew – majority of students are younger; some older students create a long right tail.

* **Unemployment rate (0.210)**: Slightly right-skewed – relatively symmetric distribution, close to normal.

* **Inflation rate (0.262)**: Slightly right-skewed – fairly balanced distribution with minor positive skew.

* **GDP (-0.407)**: Moderate left-skew – more observations above the mean; some countries have notably lower GDP values.

### **Summary satistics of Categorical Features**
"""

categorical_data = df[cat_feats]
categorical_data.describe().T

categorical_data.var()

categorical_data.skew()

categorical_data.nunique()

df[num_feats].hist(figsize=(14,14), bins=25)
plt.tight_layout(); plt.show()

# Box‑plots
plt.figure(figsize=(14, 2*len(num_feats)))
for i, col in enumerate(num_feats, 1):
    plt.subplot(len(num_feats), 1, i)
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(col)
plt.tight_layout(); plt.show()

"""## Enhanced Distribution & Outlier Analysis

### 1. Binary / Flag Variables  
- **Displaced, Debtor, Educational special needs, Scholarship holder, Tuition fees up to date, Gender, International**  
  - All exhibit a **0–1 split** with one dominant class (> 80 %).  
  - **Actionable insight:** The minority class (≈ 10–20 % of records) may carry strong signals (e.g., “scholarship holder” often correlates with outcome). Use class-weighted algorithms or generate synthetic samples if needed.

### 2. Low-Cardinality Codes & Ordinal Features  
- **Application mode**: three peaks around values ≈ 0, 20, 40  
- **Application order**: heavy concentration at 0–2, but outliers up to 9  
- **Course**, **Previous qualification**, **Parental qualification/occupation** codes:  
  - These cluster around a handful of numeric codes (e.g. Course IDs near 9000–10000, qualification levels at {0, 10, 20, 30, 40}).  
  - **Outliers** (e.g. rare Course IDs below 2000 or occupation codes > 100) should be binned into “Other” or aggregated categories to reduce sparsity.

### 3. Academic Grades  
- **Previous qualification (grade)** and **Admission grade**  
  - Approximate **normal distributions** centered ~135 with moderate right skew.  
  - **Outliers** below 100 or above 170/180 are real but rare—likely nonstandard grading.  
  - **Recommendation:** Clip or Winsorize extreme grades, or keep them but apply robust scaling in your pipeline.

### 4. Age at Enrollment  
- Strong peak at **18–22** years (traditional students) and a **long tail** to ~65 years (mature entrants).  
- **Boxplot** shows many mild outliers beyond ~30 years.  
- **Modeling tip:** Consider grouping age into bins (e.g., 18–22, 23–30, 31+) to capture nonlinear effects and reduce noise from extreme ages.

### 5. Macro-Economic Indicators  
- **Unemployment rate** (≈ 8–16 %), **Inflation rate** (≈ –1–3 %), **GDP** (normalized to ±3):  
  - These appear as **discrete bars**, reflecting country-level data snapped to each student.  
  - **Action:** If multiple students share identical macro values, these features may act more like group identifiers than true continuous variables—consider one-hot encoding of country or clustering by macro-economic segment.

### 6. Outlier Summary  
- Features with **extreme single-point outliers** (e.g., Course IDs near 0, occupation codes > 150) should be reviewed:  
  - Are they data entry errors, special programs, or genuine rare cases?  
  - You can drop or recode these into an “Other” category to improve model stability.

---

**Categorical Level Counts**
"""

cat_feats

# ↳ code
for col in cat_feats:
    plt.figure(figsize=(6,3))
    sns.countplot(x=col, data=df, order=df[col].value_counts().index)
    plt.xticks(rotation=0); plt.title(col); plt.show()
print(cat_feats)

"""# **Data‑Pre‑Processing Pipeline**
*Handles missing values → encodes categoricals → scales numerics.*
"""

drop_col = ['Target',"Father's qualification","Father's occupation","Mother's occupation"]

cat_feats = [
    "Marital status",
    "Application mode",
    "Course",
    "Daytime/evening attendance\t",
    "Previous qualification",
    "Nacionality",
    "Mother's qualification",
    # "Father's qualification",
    # "Mother's occupation",
    # "Father's occupation",
    "Displaced",
    "Educational special needs",
    "Debtor",
    "Tuition fees up to date",
    "Gender",
    "Scholarship holder",
    "International"
]

num_feats = [
    "Application order",
    "Previous qualification (grade)",
    "Admission grade",
    "Age at enrollment",
    "Unemployment rate",
    "Inflation rate",
    "GDP"
]

# ↳ code
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# ▶ drop rows with missing Target
df = df.dropna(subset=['Target']).reset_index(drop=True)

X = df.drop(drop_col, axis=1)
y = df['Target']

numeric_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot',  OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_pipeline, num_feats),
        ('cat', categorical_pipeline, cat_feats)
    ])

# train / test split for downstream modelling
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42)

# final ready‑to‑fit pipeline
print(preprocessor)

"""# 8  Key Take‑aways
* Dataset has **24 predictor features** & **3 target classes**  
* It is a **classification** task (categorical ‘Target’).  
* Classes are **imbalanced** (Graduate > Dropout > Enrolled).  
* Highest linear correlations: e.g., ‘Admission grade’ vs ‘Previous qualification (grade)’.  
* Missing values present – handled via median / mode imputation.  
* Complete preprocessing pipeline built for reproducible research.

# **Neural network**
"""

!pip install -q xgboost scikeras[tensorflow]  # silent install (TF 2.x ships with Colab)

import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from sklearn.metrics import (confusion_matrix, ConfusionMatrixDisplay,
                             classification_report, roc_auc_score, roc_curve,
                             RocCurveDisplay)
from sklearn.preprocessing import LabelEncoder, label_binarize
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import tensorflow as tf
from scikeras.wrappers import KerasClassifier   # lets us keep a sklearn‑style API

# ↳ code
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc  = le.transform(y_test)

num_classes = len(le.classes_)
y_train_bin = label_binarize(y_train_enc, classes=range(num_classes))
y_test_bin  = label_binarize(y_test_enc,  classes=range(num_classes))

print("Classes:", le.classes_)

"""Build model‑factory so scikeras can re‑create it inside cross‑validation, etc.

Combine with the **exact same** `preprocessor`
"""

# ⬅ 1.  Make sure we have compatible library versions
#!pip install -q -U scikit-learn scikeras  # (installs sklearn >=1.3, scikeras >=0.12)

import tensorflow as tf, sklearn, scikeras
print("TF :", tf.__version__, "| sklearn :", sklearn.__version__, "| scikeras :", scikeras.__version__)

# ⬅ 2.  Fit the preprocessor once
preprocessor.fit(X_train)
n_inputs = preprocessor.transform(X_train.iloc[[0]]).shape[1]

# ⬅ 3.  Build the Keras model factory
def make_dense_nn(n_inputs, n_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(n_inputs,)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(n_classes, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

from scikeras.wrappers import KerasClassifier
from sklearn.pipeline    import Pipeline

nn_clf = KerasClassifier(
    model=make_dense_nn,
    model__n_inputs=n_inputs,
    model__n_classes=num_classes,
    epochs=100,
    batch_size=64,
    validation_split=0.15,                 # ← gives EarlyStopping its 'val_loss'
    verbose=0,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=8, restore_best_weights=True)
    ]
)

nn_pipe = Pipeline(steps=[
    ('prep', preprocessor),    # already fitted but harmless to reuse
    ('nn',   nn_clf)
])


history = nn_pipe.fit(X_train, y_train_enc)

"""Evaluate NN"""

# ↳ code
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
keras_model = nn_pipe.named_steps['nn'].model_
# 1️⃣  Pre-transform the test set once (already done)
X_test_trans = preprocessor.transform(X_test)

# 2️⃣  Pure Keras prediction
y_nn_pred_prob = keras_model.predict(X_test_trans, verbose=0)
y_nn_pred      = y_nn_pred_prob.argmax(axis=1)

# 3️⃣  Standard classification report & confusion matrix
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
print("Neural Network")
print(classification_report(y_test_enc, y_nn_pred, target_names=le.classes_))

cm = confusion_matrix(y_test_enc, y_nn_pred)
ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Blues')
plt.title('NN – Confusion Matrix')
plt.show()

# 4️⃣  Compute micro-average ROC curve and AUC
y_true_flat  = y_test_bin.ravel()         # flatten one-vs-rest true labels
y_score_flat = y_nn_pred_prob.ravel()     # flatten predicted probabilities

fpr_micro, tpr_micro, _ = roc_curve(y_true_flat, y_score_flat)
auc_micro = roc_auc_score(
    y_test_bin, y_nn_pred_prob,
    multi_class='ovr',
    average='micro'
)

# 5️⃣  Plot the single micro-average ROC
plt.figure(figsize=(6, 6))
plt.plot(fpr_micro, tpr_micro,
         label=f"Micro-avg ROC (AUC = {auc_micro:.2f})")
plt.plot([0, 1], [0, 1], 'k--', label='Chance')
plt.title("Neural Network – Micro-Average ROC")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

"""# **4   Random Forest Pipeline**"""

# ↳ code
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_auc_score,
    roc_curve
)
import matplotlib.pyplot as plt


rf_pipe = Pipeline([
    ('prep', preprocessor),   #  fitted ColumnTransformer
    ('rf', RandomForestClassifier(
        n_estimators=500,
        max_depth=None,
        class_weight='balanced',
        random_state=42,
        n_jobs=-1
    ))
])
rf_pipe.fit(X_train, y_train_enc)

# 2️⃣  Generate predictions
y_rf_pred      = rf_pipe.predict(X_test)
y_rf_pred_prob = rf_pipe.predict_proba(X_test)

# 3️⃣  Standard classification metrics
print("Random Forest (CPU)")
print(classification_report(y_test_enc, y_rf_pred, target_names=le.classes_))

cm = confusion_matrix(y_test_enc, y_rf_pred)
ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Greens')
plt.title("RF – Confusion Matrix")
plt.show()

# 4️⃣  Compute a single, micro-average ROC curve
# Flatten true labels and probabilities for one-vs-rest micro-avg
y_true_flat  = y_test_bin.ravel()
y_score_flat = y_rf_pred_prob.ravel()

fpr_micro, tpr_micro, _ = roc_curve(y_true_flat, y_score_flat)
auc_micro = roc_auc_score(
    y_test_bin, y_rf_pred_prob,
    multi_class='ovr', average='micro'
)

# 5️⃣  Plot the single ROC
plt.figure(figsize=(6, 6))
plt.plot(fpr_micro, tpr_micro,
         label=f"Micro-avg ROC (AUC = {auc_micro:.2f})")
plt.plot([0, 1], [0, 1], 'k--', label='Chance')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest – Micro-Average ROC")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

"""# **5   XGBoost Pipeline**"""

# ↳ code
# 1️⃣  Imports
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    roc_auc_score
)

# 2️⃣  Define the GPU‐accelerated XGBoost classifier
xgb_gpu = XGBClassifier(
    tree_method="gpu_hist",       # use GPU for training
    predictor="gpu_predictor",    # use GPU for inference
    objective="multi:softprob",
    num_class=num_classes,
    n_estimators=600,
    learning_rate=0.04,
    max_depth=6,
    subsample=0.9,
    colsample_bytree=0.9,
    eval_metric="mlogloss",
    random_state=42
)

# 3️⃣  Build the pipeline
xgb_pipe_gpu = Pipeline([
    ('prep', preprocessor),   # your fitted ColumnTransformer
    ('xgb',  xgb_gpu)
])

# 4️⃣  Train on the CPU (preprocessing) + GPU (model)
xgb_pipe_gpu.fit(X_train, y_train_enc)

# 5️⃣  Predict on test set
y_xgb_pred      = xgb_pipe_gpu.predict(X_test)
y_xgb_pred_prob = xgb_pipe_gpu.predict_proba(X_test)

# 6️⃣  Classification report
print("\nXGBoost (GPU)")
print(classification_report(y_test_enc, y_xgb_pred, target_names=le.classes_))

# 7️⃣  Confusion matrix
cm = confusion_matrix(y_test_enc, y_xgb_pred)
ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Oranges')
plt.title("XGBoost (GPU) – Confusion Matrix")
plt.show()

# 8️⃣  Single micro-average ROC curve
# Flatten for one-vs-rest micro-avg
y_true_flat  = y_test_bin.ravel()
y_score_flat = y_xgb_pred_prob.ravel()

fpr_micro, tpr_micro, _ = roc_curve(y_true_flat, y_score_flat)
auc_micro = roc_auc_score(
    y_test_bin,
    y_xgb_pred_prob,
    multi_class="ovr",
    average="micro"
)

plt.figure(figsize=(6, 6))
plt.plot(fpr_micro, tpr_micro, label=f"Micro-avg ROC (AUC = {auc_micro:.2f})")
plt.plot([0, 1], [0, 1], 'k--', label="Chance")
plt.title("XGBoost (GPU) – Micro-Average ROC")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

"""# **KNN**"""

# ↳ code
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline

# 1️⃣  Build & train KNN pipeline
knn_pipe = Pipeline([
    ('prep', preprocessor),
    ('knn', KNeighborsClassifier(n_neighbors=5, weights='uniform', n_jobs=-1))
])
knn_pipe.fit(X_train, y_train_enc)

# 2️⃣  Predict & evaluate
y_knn_pred      = knn_pipe.predict(X_test)
y_knn_pred_prob = knn_pipe.predict_proba(X_test)

print("K-Nearest Neighbors")
print(classification_report(y_test_enc, y_knn_pred, target_names=le.classes_))

cm = confusion_matrix(y_test_enc, y_knn_pred)
ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Purples')
plt.title("KNN – Confusion Matrix")
plt.show()

# 3️⃣  Micro-average ROC
y_true_flat  = y_test_bin.ravel()
y_score_flat = y_knn_pred_prob.ravel()
fpr_micro, tpr_micro, _ = roc_curve(y_true_flat, y_score_flat)
auc_micro = roc_auc_score(y_test_bin, y_knn_pred_prob, multi_class='ovr', average='micro')

plt.figure(figsize=(6,6))
plt.plot(fpr_micro, tpr_micro, label=f"Micro-avg ROC (AUC = {auc_micro:.2f})")
plt.plot([0,1],[0,1],'k--', label='Chance')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("KNN – Micro-Average ROC")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

"""# **Decision Tree**"""

# ↳ code
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline

# 1️⃣  Build & train Decision Tree pipeline
dt_pipe = Pipeline([
    ('prep', preprocessor),
    ('dt', DecisionTreeClassifier(
        criterion='gini',
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        class_weight='balanced',
        random_state=42
    ))
])
dt_pipe.fit(X_train, y_train_enc)

# 2️⃣  Predict & evaluate
y_dt_pred      = dt_pipe.predict(X_test)
y_dt_pred_prob = dt_pipe.predict_proba(X_test)

print("Decision Tree")
print(classification_report(y_test_enc, y_dt_pred, target_names=le.classes_))

cm = confusion_matrix(y_test_enc, y_dt_pred)
ConfusionMatrixDisplay(cm, display_labels=le.classes_).plot(cmap='Oranges')
plt.title("Decision Tree – Confusion Matrix")
plt.show()

# 3️⃣  Micro-average ROC
y_true_flat  = y_test_bin.ravel()
y_score_flat = y_dt_pred_prob.ravel()
fpr_micro, tpr_micro, _ = roc_curve(y_true_flat, y_score_flat)
auc_micro = roc_auc_score(y_test_bin, y_dt_pred_prob, multi_class='ovr', average='micro')

plt.figure(figsize=(6,6))
plt.plot(fpr_micro, tpr_micro, label=f"Micro-avg ROC (AUC = {auc_micro:.2f})")
plt.plot([0,1],[0,1],'k--', label='Chance')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Decision Tree – Micro-Average ROC")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

"""# **Logistic Regression**"""

# ↳ code
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, label_binarize
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report,
    confusion_matrix, ConfusionMatrixDisplay,
    roc_curve, roc_auc_score
)

# 1) Drop the target and the two parental‐qualification cols from df
df2 = df.drop(drop_col, axis=1)
y   = df['Target']

# 2) Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    df2, y, test_size=0.3, stratify=y, random_state=42
)

# 3) Define your feature lists (minus the dropped ones!)

# 4) Build a preprocessor that outputs a **dense** array
numeric_pipeline = Pipeline([
    ('impute', SimpleImputer(strategy='median')),
    ('scale',   StandardScaler())
])
categorical_pipeline = Pipeline([
    ('impute', SimpleImputer(strategy='most_frequent')),
    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])
preprocessor = ColumnTransformer([
    ('num', numeric_pipeline,   num_feats),
    ('cat', categorical_pipeline, cat_feats)
])

# 5) Fit & transform
X_train_pre = preprocessor.fit_transform(X_train)
X_test_pre  = preprocessor.transform(X_test)

# 6) Encode the target
le = LabelEncoder().fit(y_train)
y_train_enc = le.transform(y_train)
y_test_enc  = le.transform(y_test)

# 7) Binarize for ROC-AUC
y_train_bin = label_binarize(y_train_enc, classes=range(len(le.classes_)))
y_test_bin  = label_binarize(y_test_enc,  classes=range(len(le.classes_)))

# 8) Train logistic regression
lr = LogisticRegression(
    max_iter=1000, class_weight='balanced',
    solver='lbfgs', multi_class='ovr', n_jobs=-1
)
lr.fit(X_train_pre, y_train_enc)

# 9) Predict & evaluate
y_lr_pred      = lr.predict(X_test_pre)
y_lr_prob      = lr.predict_proba(X_test_pre)

print("Logistic Regression")
print(classification_report(y_test_enc, y_lr_pred, target_names=le.classes_))

ConfusionMatrixDisplay.from_predictions(
    y_test_enc, y_lr_pred, display_labels=le.classes_, cmap='Purples'
)
plt.title("LR – Confusion Matrix")
plt.show()

# 10) Single micro-avg ROC
fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_lr_prob.ravel())
auc_micro   = roc_auc_score(y_test_bin, y_lr_prob, multi_class='ovr', average='micro')

plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f"Micro-avg ROC (AUC={auc_micro:.2f})")
plt.plot([0,1],[0,1],'k--', label="Chance")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("LR – Micro-Average ROC")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()

# ↳ code
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# 1) Collect predictions into a dict
model_preds = {
    "Neural Net":     y_nn_pred,
    "Random Forest":  y_rf_pred,
    "XGBoost":        y_xgb_pred,
    "KNN":            y_knn_pred,
    "Decision Tree":  y_dt_pred,
    "Logistic Reg":   y_lr_pred,

}

# 2) Compute metrics for each
rows = []
for name, preds in model_preds.items():
    rows.append({
        "Model":     name,
        "Accuracy":  accuracy_score(y_test_enc, preds),
        "Precision": precision_score(y_test_enc, preds, average="macro", zero_division=0),
        "Recall":    recall_score(y_test_enc, preds, average="macro", zero_division=0),
        "F1-score":  f1_score(y_test_enc, preds, average="macro", zero_division=0)
    })

metrics_df = pd.DataFrame(rows).set_index("Model")

# 3) Plot grouped bar chart
ax = metrics_df.plot(
    kind="bar",
    figsize=(10, 6),
    ylabel="Score",
    ylim=(0, 1.0),
    rot=0
)
plt.title("Model Comparison Across Metrics")
plt.legend(title="Metric", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# ↳ code
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# 1) Ensure you have these one-hot truth & probability arrays in memory:
#    y_test_bin, y_nn_pred_prob, y_rf_pred_prob,
#    y_xgb_pred_prob, y_knn_pred_prob, y_dt_pred_prob, y_lr_prob

model_probs = {
    "Neural Net":     y_nn_pred_prob,
    "Random Forest":  y_rf_pred_prob,
    "XGBoost":        y_xgb_pred_prob,
    "KNN":            y_knn_pred_prob,
    "Decision Tree":  y_dt_pred_prob,
    "Logistic Reg":   y_lr_prob,
}

# 2) Plot all micro-average ROC curves together
plt.figure(figsize=(8, 6))
plt.plot([0, 1], [0, 1], 'k--', label='Chance')

for name, prob in model_probs.items():
    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), prob.ravel())
    auc = roc_auc_score(y_test_bin, prob, multi_class='ovr', average='micro')
    plt.plot(fpr, tpr, label=f"{name} (AUC = {auc:.2f})")

plt.title("Micro-Average ROC Curves — All Models")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(alpha=0.3)
plt.show()